# LatentCore Configuration
# Copy to .env and modify as needed

# Upstream LLM endpoint (OpenAI-compatible)
LATENTCORE_UPSTREAM_BASE_URL=http://localhost:11434/v1
LATENTCORE_UPSTREAM_API_KEY=
LATENTCORE_UPSTREAM_TIMEOUT_S=120.0

# Server
LATENTCORE_HOST=0.0.0.0
LATENTCORE_PORT=8000

# VQ Codebook Engine
LATENTCORE_CODEBOOK_SIZE=8192
LATENTCORE_EMBEDDING_DIM=768
LATENTCORE_COMMITMENT_COST=0.25
LATENTCORE_EMA_DECAY=0.99

# Infini-Attention
LATENTCORE_D_KEY=64
LATENTCORE_D_VALUE=64
LATENTCORE_SEGMENT_LENGTH=2048

# Context Analysis
LATENTCORE_COMPRESS_THRESHOLD_TOKENS=1000
LATENTCORE_MAX_INPUT_TOKENS=128000

# Storage
LATENTCORE_DB_PATH=latentcore.db

# Device: auto | cpu | cuda | mps
LATENTCORE_DEVICE=auto

# Logging
LATENTCORE_LOG_LEVEL=INFO
